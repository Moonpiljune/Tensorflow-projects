{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudML with DataLAB Part. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deploy the Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! I'm back! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we're going to learn how to deploy your trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow provides the function that you can serve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know it's tooooooooooo difficault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But!!! If you deploy using CloudML, That comes to be so easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to build the simple Neural Network Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would take 10dims input vector like [0, 1, 2, .... 9] and [9, 8, 7, .. , 0] so predicts whether increase or decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80873543]\n",
      " [ 0.999708  ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = [[i for i in range(10)], [9-i for i in range(10)]]\n",
    "x = tf.placeholder(tf.float32, [None, 10])\n",
    "feed_dict = {x: data}\n",
    "\n",
    "layer1 = tf.layers.dense(x, 10)\n",
    "layer2 = tf.layers.dense(layer1, 10)\n",
    "layer3 = tf.layers.dense(layer2, 10)\n",
    "logits = tf.layers.dense(layer3, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  out = sess.run(tf.nn.sigmoid(logits), feed_dict=feed_dict)\n",
    "  print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it's not a trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will take the neural network model from this code, and train on CloudML at first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a data file to simulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has written\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('DataSet/train_data.csv', 'w') as f:\n",
    "  writer = csv.writer(f)\n",
    "  for i in range(1000):\n",
    "    if i % 2 == 0:\n",
    "      writer.writerow([1]+[i for i in range(10)])\n",
    "    else:\n",
    "      writer.writerow([0]+[9-i for i in range(10)])\n",
    "print(\"data has written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checke whether it saved well by reading it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the element in column 0 means label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see the 1 label means a increase data and 0 means opposit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "['0', '9', '8', '7', '6', '5', '4', '3', '2', '1', '0']\n",
      "['1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "['0', '9', '8', '7', '6', '5', '4', '3', '2', '1', '0']\n",
      "['1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "['0', '9', '8', '7', '6', '5', '4', '3', '2', '1', '0']\n",
      "['1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "['0', '9', '8', '7', '6', '5', '4', '3', '2', '1', '0']\n",
      "['1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "['0', '9', '8', '7', '6', '5', '4', '3', '2', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "with open('DataSet/train_data.csv', 'r') as f:\n",
    "  reader = csv.reader(f)\n",
    "  data = list(reader)\n",
    "  for i in range(10):\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll copy it to my bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://./DataSet/train_data.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/ 22.5 KiB]                                                \r",
      "/ [1 files][ 22.5 KiB/ 22.5 KiB]                                                \r\n",
      "Operation completed over 1 objects/22.5 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROJECT_ID=tensorflowprojects\n",
    "BUCKET=\"gs://${PROJECT_ID}-mlengine\"\n",
    "\n",
    "gsutil cp -r ./DataSet ${BUCKET}/DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to see the huge code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't freak out! You can use this code by copy and change a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our purpose is setting the Experiment function to run learn_runner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the details about this experiment function (document in tensorflow.org)\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the expriment function takes input arguments such Input function, Estimator, Metrics, Serving function which are defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to write this code in simple_model.py file using the magic %%writefile line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the commants to refer the detal of each functions if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/simple_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model/simple_model.py\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import metrics\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#\n",
    "# 1. Input function\n",
    "#\n",
    "def input_fn(filename, batch_size=20):\n",
    "  \"\"\"\n",
    "  We use this function to read the train data and build the batch\n",
    "  If you have used to use the queue runners, go ahead at same\n",
    "  Just care for the returns.\n",
    "  you can see the return dictionary {'data': b_train}\n",
    "  this form is for feed data from external query\n",
    "  When you call the model to predict, you would use key for input name, value for input data to predict. That's why\n",
    "  \"\"\"\n",
    "  filename_queue = tf.train.string_input_producer([filename])\n",
    "  reader = tf.TextLineReader()\n",
    "  _, value = reader.read(filename_queue)\n",
    "  dataset = tf.decode_csv(value, [[0]]*11)\n",
    "  label = tf.cast(dataset[0], tf.float32)\n",
    "  train = tf.cast(tf.stack(dataset[1:]), tf.float32)\n",
    "\n",
    "  b_train, b_label = tf.train.batch([train, label], batch_size)\n",
    "  return {'data':b_train}, tf.reshape(b_label, (-1, 1))\n",
    "\n",
    "\n",
    "def get_input_fn(filename, batch_size):\n",
    "  \"\"\"\n",
    "  and you see the this function returns the input_fn as function type. \n",
    "  it's not a big deal. it's required by experiment function\n",
    "  \"\"\"\n",
    "  return lambda: input_fn(filename, batch_size)\n",
    "\n",
    "\n",
    "#\n",
    "# 2. Estimator\n",
    "#\n",
    "def _simple_model_fn(features, labels, mode):\n",
    "  \"\"\"\n",
    "  This function is just a model that you built\n",
    "  you can see the input argument 'features'\n",
    "  Actually it's a placeholders wrapped by dictionary \n",
    "  It's considered that any user who call this model inputs a data as a dict type to predict\n",
    "  \"\"\"\n",
    "  x = features['data']\n",
    "  layer1 = tf.layers.dense(x, 10)\n",
    "  layer2 = tf.layers.dense(layer1, 10)\n",
    "  layer3 = tf.layers.dense(layer2, 10)\n",
    "  logits = tf.layers.dense(layer3, 1)\n",
    "\n",
    "  loss = None\n",
    "  train_op = None\n",
    "\n",
    "  \n",
    "  \"\"\"\n",
    "  Mode key means whether this model is training or not\n",
    "  You can make a train_op and calculate loss like below\n",
    "  \"\"\"\n",
    "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "  if mode != learn.ModeKeys.INFER:\n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == learn.ModeKeys.TRAIN:\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss=loss,\n",
    "      global_step=tf.contrib.framework.get_global_step(),\n",
    "      learning_rate=0.001, optimizer=\"Adam\")\n",
    "\n",
    "  \"\"\"\n",
    "  Then you can make the prediction form and return ModelFnOp object like below\n",
    "  \"\"\"\n",
    "  # Generate Predictions\n",
    "  predictions = {\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      \"probabilities\": tf.nn.sigmoid(logits)\n",
    "  }\n",
    "\n",
    "  # Return a ModelFnOps object\n",
    "  return model_fn_lib.ModelFnOps(mode=mode, loss=loss, train_op=train_op,\n",
    "                               predictions=predictions)\n",
    "\n",
    "\n",
    "def build_estimator(model_dir):\n",
    "  \"\"\"\n",
    "  Lastly, build the estimator. Yeah just copy and paste. who knows?\n",
    "  \"\"\"\n",
    "  return learn.Estimator(\n",
    "         model_fn=_simple_model_fn,\n",
    "         model_dir=model_dir,\n",
    "         config=tf.contrib.learn.RunConfig(save_checkpoints_secs=180))\n",
    "\n",
    "\n",
    "#\n",
    "# 3. Evaluate metrics\n",
    "#\n",
    "def get_eval_metrics():\n",
    "  \"\"\"\n",
    "  This function returns which metrics would like you see for evaluating\n",
    "  These links would be help\n",
    "  https://www.tensorflow.org/api_docs/python/tf/contrib/learn/MetricSpec\n",
    "  https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy\n",
    "  \"\"\"\n",
    "  return {\"accuracy\": learn.MetricSpec(metric_fn=tf.metrics.accuracy,\n",
    "                                     prediction_key=\"classes\")\n",
    "         }\n",
    "\n",
    "#\n",
    "# 4. Serving function\n",
    "#\n",
    "def serving_input_fn():\n",
    "  \"\"\"\n",
    "  You saw the placeholder dictionary like {'data': tensor } form. \n",
    "  'data' is the name of input. I will be query to model with input data like {'data': [0, 1, 2, 3..., 9]}\n",
    "  You can make the number of inputs in dictionary here\n",
    "  Make the feature_placeholders in dictionary like below, and copy and paste otherwise. who knows!!\n",
    "  \"\"\"\n",
    "  feature_placeholders = {'data': tf.placeholder(tf.float32, [None, 10])}\n",
    "  features = {\n",
    "    key: tensor\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }    \n",
    "  return learn.utils.input_fn_utils.InputFnOps(\n",
    "    features,\n",
    "    None,\n",
    "    feature_placeholders\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last! All functions are defined for experiment function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the full code to build the experiment object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to change them hardly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just make experiment object following this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model/task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import simple_model as model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn import Experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "\n",
    "def generate_experiment_fn(data_dir=None,\n",
    "                           train_batch_size=100,\n",
    "                           eval_batch_size=100,\n",
    "                           train_steps=10000,\n",
    "                           eval_steps=100,\n",
    "                           **experiment_args):\n",
    "  \n",
    "  def _experiment_fn(output_dir):\n",
    "    return Experiment(\n",
    "      model.build_estimator(output_dir),\n",
    "      train_input_fn=model.get_input_fn(\n",
    "        filename=os.path.join(data_dir, \"train_data.csv\"),\n",
    "        batch_size=train_batch_size),\n",
    "      eval_input_fn=model.get_input_fn(\n",
    "        filename=os.path.join(data_dir, \"train_data.csv\"),\n",
    "        batch_size=eval_batch_size),\n",
    "\n",
    "      export_strategies=[saved_model_export_utils.make_export_strategy(\n",
    "          model.serving_input_fn,\n",
    "          default_output_alternative_key=None,\n",
    "          exports_to_keep=1\n",
    "        )],\n",
    "      train_steps=train_steps,\n",
    "      eval_metrics=model.get_eval_metrics(),\n",
    "      eval_steps=eval_steps,\n",
    "      **experiment_args\n",
    "      )\n",
    "  return _experiment_fn\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--data_dir',\n",
    "      help='GCS or local path to training data',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_batch_size',\n",
    "      help='Batch size for training steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_batch_size',\n",
    "      help='Batch size for evaluation steps',\n",
    "      type=int,\n",
    "      default=100\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--train_steps',\n",
    "      help='Steps to run the training job for.',\n",
    "      type=int,\n",
    "      default=10000\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_steps',\n",
    "      help='Number of steps to run evalution for at each checkpoint',\n",
    "      default=100,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='this model ignores this field, but it is required by gcloud',\n",
    "      default='junk'\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--eval_delay_secs',\n",
    "      help='How long to wait before running first evaluation',\n",
    "      default=10,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--min_eval_frequency',\n",
    "      help='Minimum number of training steps between evaluations',\n",
    "      default=1,\n",
    "      type=int\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  arguments = args.__dict__\n",
    "\n",
    "  # unused args provided by service\n",
    "  arguments.pop('job_dir', None)\n",
    "  arguments.pop('job-dir', None)\n",
    "\n",
    "  output_dir = arguments.pop('output_dir')\n",
    "\n",
    "  # Run the training job\n",
    "  learn_runner.run(generate_experiment_fn(**arguments), output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see we've done right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm whether the model has an error using CloudML in local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this shell commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: -- \\ argument splites the input arguments in two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left sides are for ml-engint, othersides are for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 180, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f334ddebad0>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:322: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into gs://tensorflowprojects-mlengine/simple_code_in_cloudml_test_20170611_045113/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.73212, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-11-04:51:53\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-11-04:51:59\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.5, global_step = 1, loss = 3.51638\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 1): loss = 3.51638, global_step = 1, accuracy = 0.5\n",
      "INFO:tensorflow:Saving checkpoints for 3 into gs://tensorflowprojects-mlengine/simple_code_in_cloudml_test_20170611_045113/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.30508.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-11-04:52:11\n",
      "INFO:tensorflow:Evaluation [1/3]\n",
      "INFO:tensorflow:Evaluation [2/3]\n",
      "INFO:tensorflow:Evaluation [3/3]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-11-04:52:19\n",
      "INFO:tensorflow:Saving dict for global step 3: accuracy = 0.5, global_step = 3, loss = 3.09828\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "WARNING:tensorflow:export_savedmodel (from tensorflow.contrib.learn.python.learn.estimators.estimator) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1285: initialize_local_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.local_variables_initializer` instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: gs://tensorflowprojects-mlengine/simple_code_in_cloudml_test_20170611_045113/export/Servo/1497156741924/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROJECT_ID=tensorflowprojects\n",
    "BUCKET=\"gs://${PROJECT_ID}-mlengine\"\n",
    "JOB_NAME=simple_code_in_cloudml_test_$(date +%Y%m%d_%H%M%S)\n",
    "PACKAGE_PATH=$(pwd)/model\n",
    "MODULE_NAME=model.task\n",
    "STAGING_BUCKET=${BUCKET}\n",
    "JOB_DIR=${BUCKET}/${JOB_NAME}\n",
    "OUTPUT=${BUCKET}/${JOB_NAME}\n",
    "REGION=europe-west1\n",
    "SCALE_TIER=BASIC_GPU\n",
    "\n",
    "# Submit job with these settings\n",
    "gcloud ml-engine local train \\\n",
    "--package-path=$PACKAGE_PATH \\\n",
    "--module-name=$MODULE_NAME \\\n",
    "-- \\\n",
    "--data_dir=${BUCKET}/DataSet \\\n",
    "--output_dir=${BUCKET}/${JOB_NAME} \\\n",
    "--train_step=3 \\\n",
    "--eval_step=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it can be submitted by JOB on CloudML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your JOB following commands in JOB and waiting for it training done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: simple_model_20170614_230005\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [simple_model_20170614_230005] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe simple_model_20170614_230005\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs simple_model_20170614_230005\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROJECT_ID=tensorflowprojects\n",
    "BUCKET=\"gs://${PROJECT_ID}-mlengine\"\n",
    "JOB_NAME=simple_model_$(date +%Y%m%d_%H%M%S)\n",
    "PACKAGE_PATH=$(pwd)/model\n",
    "MODULE_NAME=model.task\n",
    "STAGING_BUCKET=${BUCKET}\n",
    "JOB_DIR=${BUCKET}/${JOB_NAME}\n",
    "OUTPUT=${BUCKET}/${JOB_NAME}\n",
    "REGION=europe-west1\n",
    "SCALE_TIER=BASIC_GPU\n",
    "\n",
    "# Submit job with these settings\n",
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "--package-path=$PACKAGE_PATH \\\n",
    "--module-name=$MODULE_NAME \\\n",
    "--staging-bucket=$STAGING_BUCKET \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--region=$REGION \\\n",
    "-- \\\n",
    "--data_dir=${BUCKET}/DataSet \\\n",
    "--output_dir=${BUCKET}/${JOB_NAME} \\\n",
    "--train_step=100 \\\n",
    "--eval_step=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, mine is Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, only deploy the model is remained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find your model following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://tensorflowprojects-mlengine/simple_model_20170611_050925/export/Servo/1497157932533/\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "echo $(gsutil ls gs://tensorflowprojects-mlengine/simple_model_20170611_050925/export/Servo | tail -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, Deploy! two lines are enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating version (this might take a few minutes)......\n",
      "..........................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "REGION='us-central1'\n",
    "MODEL_NAME=\"simple_model\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://tensorflowprojects-mlengine/simple_model_20170611_050925/export/Servo | tail -1)\n",
    "\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all you have done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction model you've deployed can be called by query with json file which input data is written down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have any input data so I made it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file written\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"request.json\", \"w\") as f:\n",
    "  for i in range(10):\n",
    "    if i < 5:\n",
    "      f.write(json.dumps({\"inputs\": [9-i for i in range(10)]}) + '\\n')\n",
    "    else:\n",
    "      f.write(json.dumps({\"inputs\": [i for i in range(10)]}) + '\\n')\n",
    "      \n",
    "print(\"json file written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, request prediction model using this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSES  PROBABILITIES\n",
      "0        [1.2494094789872179e-06]\n",
      "0        [1.2494094789872179e-06]\n",
      "0        [1.2494094789872179e-06]\n",
      "0        [1.2494094789872179e-06]\n",
      "0        [1.2494094789872179e-06]\n",
      "0        [0.9999992847442627]\n",
      "0        [0.9999992847442627]\n",
      "0        [0.9999992847442627]\n",
      "0        [0.9999992847442627]\n",
      "0        [0.9999992847442627]\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "MODEL_NAME=\"simple_model\"\n",
    "gcloud ml-engine predict --model ${MODEL_NAME} --json-instances request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got this!!! You all have done!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSES is for multi-class classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this model predicts by binary so 0 value of CLASSES is right to be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all. I think this part was pretty hard to follow than previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some references to help following this part easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CloudML Prediction API\n",
    "https://github.com/GoogleCloudPlatform/cloudml-dist-mnist-example\n",
    "\n",
    "deploy document https://cloud.google.com/ml-engine/docs/concepts/prediction-overview\n",
    "\n",
    "exmaple https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/babyweight/babyweight.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
