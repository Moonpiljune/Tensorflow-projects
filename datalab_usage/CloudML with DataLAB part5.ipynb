{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudML with DataLAB Part. 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for comming again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session, we would learn how to train the model as distributed learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems so difficault, actually not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with simple code at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, distributed TensorFlow!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "c = tf.constant(\"Hello, distributed TensorFlow!\")\n",
    "server = tf.train.Server.create_local_server()\n",
    "\n",
    "# create a session on the server\n",
    "with tf.Session(server.target) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed learning means that model is trained by the distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distrubute means the many of computers for training exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, right! That's why the server is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, many of computers interact each other to recieve the data to calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I created a server and opened the session in the server.target This is the simplest code of distributed learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the local computer (this instance) runs only.  If I have a CPU and a GPU, I just can use resource from them. only them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... is that what you want?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use more as we can! check out the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model/test.py\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# The environment variable \"TF_CONFIG\" will be defined when the ml-engine runs\n",
    "# but returns None before \n",
    "tf_config = os.environ.get('TF_CONFIG')\n",
    "\n",
    "# you can read it as json.\n",
    "tf_config_json = json.loads(tf_config)\n",
    "\n",
    "# We need the cluster spec to create distributed server\n",
    "# What we get like below is included in the spec\n",
    "cluster = tf_config_json.get('cluster')\n",
    "job_name = tf_config_json.get('task', {}).get('type')\n",
    "task_index = tf_config_json.get('task', {}).get('index')\n",
    "\n",
    "# Let's see what there is\n",
    "pprint(tf_config_json)\n",
    "print \"==========================================================\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course there is nothing to see if you run this code by python interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the server count, worker count and whether distributed then run the ml-engine so TF_CONFIG would be set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see the result like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'cluster': {u'master': [u'https://27182-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'ps': [u'https://27183-dot-2183444-dot-devshell.appspot.com', u'https://27184-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'worker': [u'https://27185-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27186-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27187-dot-2183444-dot-devshell.appspot.com']},\n",
      " u'environment': u'cloud',\n",
      " u'job': {u'args': [], u'job_name': u'model.test'},\n",
      " u'task': {u'index': 2, u'type': u'worker'}}\n",
      "==========================================================\n",
      "{u'cluster': {u'master': [u'https://27182-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'ps': [u'https://27183-dot-2183444-dot-devshell.appspot.com', u'https://27184-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'worker': [u'https://27185-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27186-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27187-dot-2183444-dot-devshell.appspot.com']},\n",
      " u'environment': u'cloud',\n",
      " u'job': {u'args': [], u'job_name': u'model.test'},\n",
      " u'task': {u'index': 0, u'type': u'master'}}\n",
      "==========================================================\n",
      "{u'cluster': {u'master': [u'https://27182-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'ps': [u'https://27183-dot-2183444-dot-devshell.appspot.com', u'https://27184-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'worker': [u'https://27185-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27186-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27187-dot-2183444-dot-devshell.appspot.com']},\n",
      " u'environment': u'cloud',\n",
      " u'job': {u'args': [], u'job_name': u'model.test'},\n",
      " u'task': {u'index': 0, u'type': u'ps'}}\n",
      "==========================================================\n",
      "{u'cluster': {u'master': [u'https://27182-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'ps': [u'https://27183-dot-2183444-dot-devshell.appspot.com', u'https://27184-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'worker': [u'https://27185-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27186-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27187-dot-2183444-dot-devshell.appspot.com']},\n",
      " u'environment': u'cloud',\n",
      " u'job': {u'args': [], u'job_name': u'model.test'},\n",
      " u'task': {u'index': 0, u'type': u'worker'}}\n",
      "==========================================================\n",
      "{u'cluster': {u'master': [u'https://27182-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'ps': [u'https://27183-dot-2183444-dot-devshell.appspot.com', u'https://27184-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'worker': [u'https://27185-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27186-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27187-dot-2183444-dot-devshell.appspot.com']},\n",
      " u'environment': u'cloud',\n",
      " u'job': {u'args': [], u'job_name': u'model.test'},\n",
      " u'task': {u'index': 1, u'type': u'ps'}}\n",
      "==========================================================\n",
      "{u'cluster': {u'master': [u'https://27182-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'ps': [u'https://27183-dot-2183444-dot-devshell.appspot.com', u'https://27184-dot-2183444-dot-devshell.appspot.com'],\n",
      "              u'worker': [u'https://27185-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27186-dot-2183444-dot-devshell.appspot.com',\n",
      "                          u'https://27187-dot-2183444-dot-devshell.appspot.com']},\n",
      " u'environment': u'cloud',\n",
      " u'job': {u'args': [], u'job_name': u'model.test'},\n",
      " u'task': {u'index': 1, u'type': u'worker'}}\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud ml-engine local train \\\n",
    "--package-path=$(pwd)/model \\\n",
    "--module-name=model.test \\\n",
    "--parameter-server-count=2 \\\n",
    "--worker-count=3 \\\n",
    "--distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could be curious what worker is or ps is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just take it easy to say that ps is CPU and worker is GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps is for operation of model, worker is for calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details are in video [here](https://youtu.be/la_M6bCV91M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got understand what those is, check out the \"task\" in the log,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the values \"index\" \"type\". you can see how many worker are(actually they're that you set) or how many ps are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps is same as server count, worker count is variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also you can see the type \"master\". It's to be a main server that you specified to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would see how to construct the server to distributed learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model to test first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model/test.py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# get environment variable TF_CONFIG to specify the cluster\n",
    "tf_config = os.environ.get('TF_CONFIG')\n",
    "\n",
    "tf_config_json = json.loads(tf_config)\n",
    "\n",
    "cluster = tf_config_json.get('cluster')\n",
    "job_name = tf_config_json.get('task', {}).get('type')\n",
    "task_index = tf_config_json.get('task', {}).get('index')\n",
    "\n",
    "# This is how to create server for distrubted learning\n",
    "cluster_spec = tf.train.ClusterSpec(cluster)\n",
    "server = tf.train.Server(cluster_spec,\n",
    "                         job_name=job_name,\n",
    "                         task_index=task_index)\n",
    "\n",
    "# this model is built last session\n",
    "data = [[i for i in range(10)], [9-i for i in range(10)]]\n",
    "label = [[1], [0]]\n",
    "x = tf.placeholder(tf.float32, [None, 10])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "feed_dict = {x: data, y_: label}\n",
    "\n",
    "layer1 = tf.layers.dense(x, 10)\n",
    "layer2 = tf.layers.dense(layer1, 10)\n",
    "layer3 = tf.layers.dense(layer2, 10)\n",
    "logits = tf.layers.dense(layer3, 1)\n",
    "\n",
    "loss = tf.losses.sigmoid_cross_entropy(y_, logits)\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "# Run distributed learning!\n",
    "with tf.Session(server.target) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        _, _loss = sess.run([train, loss], feed_dict=feed_dict)\n",
    "        if i % 10 == 0:\n",
    "            print(\"loss: \", _loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss: ', 0.50345963)\n",
      "('loss: ', 0.00014202719)\n",
      "('loss: ', 0.00011008905)\n",
      "('loss: ', 9.104309e-05)\n",
      "('loss: ', 7.7581746e-05)\n",
      "('loss: ', 6.7566274e-05)\n",
      "('loss: ', 6.0151204e-05)\n",
      "('loss: ', 5.4369248e-05)\n",
      "('loss: ', 5.042235e-05)\n",
      "('loss: ', 4.6871926e-05)\n",
      "('loss: ', 0.50345963)\n",
      "('loss: ', 0.00014632563)\n",
      "('loss: ', 0.00011341777)\n",
      "('loss: ', 9.2545917e-05)\n",
      "('loss: ', 7.904564e-05)\n",
      "('loss: ', 6.7839814e-05)\n",
      "('loss: ', 5.9507645e-05)\n",
      "('loss: ', 5.4281023e-05)\n",
      "('loss: ', 4.9818937e-05)\n",
      "('loss: ', 4.6092333e-05)\n",
      "('loss: ', 0.50345963)\n",
      "('loss: ', 0.00014262296)\n",
      "('loss: ', 0.00011228817)\n",
      "('loss: ', 9.3577437e-05)\n",
      "('loss: ', 7.7760989e-05)\n",
      "('loss: ', 6.7566274e-05)\n",
      "('loss: ', 5.9507645e-05)\n",
      "('loss: ', 5.4192737e-05)\n",
      "('loss: ', 5.027006e-05)\n",
      "('loss: ', 4.6478945e-05)\n",
      "('loss: ', 0.50345963)\n",
      "('loss: ', 0.00014632563)\n",
      "('loss: ', 0.00011615325)\n",
      "('loss: ', 9.4101364e-05)\n",
      "('loss: ', 7.904564e-05)\n",
      "('loss: ', 6.839422e-05)\n",
      "('loss: ', 6.0368478e-05)\n",
      "('loss: ', 5.463645e-05)\n",
      "('loss: ', 5.027006e-05)\n",
      "('loss: ', 4.7271475e-05)\n",
      "('loss: ', 0.50345963)\n",
      "('loss: ', 0.00014383135)\n",
      "('loss: ', 0.00011341777)\n",
      "('loss: ', 9.3577437e-05)\n",
      "('loss: ', 7.9418809e-05)\n",
      "('loss: ', 6.8815272e-05)\n",
      "('loss: ', 6.0368478e-05)\n",
      "('loss: ', 5.4369248e-05)\n",
      "('loss: ', 5.0652332e-05)\n",
      "('loss: ', 4.6938036e-05)\n",
      "('loss: ', 2.8519149)\n",
      "('loss: ', 0.00015867356)\n",
      "('loss: ', 0.00013101548)\n",
      "('loss: ', 0.00011266215)\n",
      "('loss: ', 9.821909e-05)\n",
      "('loss: ', 8.703841e-05)\n",
      "('loss: ', 7.8860583e-05)\n",
      "('loss: ', 7.1765317e-05)\n",
      "('loss: ', 6.5838656e-05)\n",
      "('loss: ', 6.1142389e-05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27186\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27187\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27184\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27185\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27182\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27183\n",
      "I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session faa134e6ef09b1cd with config: \n",
      "\n",
      "I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session 56ce67e496171612 with config: \n",
      "\n",
      "I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session 2ed86e33ce568f18 with config: \n",
      "\n",
      "I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session cbe93d8149cc8db2 with config: \n",
      "\n",
      "I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session 13ca82909222c004 with config: \n",
      "\n",
      "I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session 3452aac47c40d6a8 with config: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud ml-engine local train \\\n",
    "--package-path=$(pwd)/model \\\n",
    "--module-name=model.test \\\n",
    "--parameter-server-count=2 \\\n",
    "--worker-count=3 \\\n",
    "--distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not the end. distributed learning is run but it's meaningless because model was run at each server as same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to saperate each variables or operation to each server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, all the ps nodes have to be set to listen data for operate and all workers have to be set to execute the calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be few changes in the session also. check this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model/test.py\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# get environment variable TF_CONFIG to specify the cluster\n",
    "tf_config = os.environ.get('TF_CONFIG')\n",
    "\n",
    "tf_config_json = json.loads(tf_config)\n",
    "\n",
    "cluster = tf_config_json.get('cluster')\n",
    "job_name = tf_config_json.get('task', {}).get('type')\n",
    "task_index = tf_config_json.get('task', {}).get('index')\n",
    "\n",
    "\n",
    "# This is how to create server for distrubted learning\n",
    "cluster_spec = tf.train.ClusterSpec(cluster)\n",
    "server = tf.train.Server(cluster_spec,\n",
    "                         job_name=job_name,\n",
    "                         task_index=task_index)\n",
    "\n",
    "data = [[i for i in range(10)], [9-i for i in range(10)]]\n",
    "label = [[1], [0]]\n",
    "\n",
    "# set ps listening\n",
    "if job_name == 'ps':\n",
    "  server.join()\n",
    "elif job_name == 'worker':\n",
    "  # seperate variables to the workers\n",
    "  with tf.device(tf.train.replica_device_setter()):\n",
    "    x = tf.placeholder(tf.float32, [None, 10])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "    feed_dict = {x: data, y_: label}\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, 10)\n",
    "    layer2 = tf.layers.dense(layer1, 10)\n",
    "    layer3 = tf.layers.dense(layer2, 10)\n",
    "    logits = tf.layers.dense(layer3, 1)\n",
    "    \n",
    "    # you cannot use tf.Session() to train the model using for .. in ... \n",
    "    # so you need to create the object which define the global step\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    \n",
    "    loss = tf.losses.sigmoid_cross_entropy(y_, logits)\n",
    "    train = tf.train.GradientDescentOptimizer(0.01).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # you can set the global step here\n",
    "  hooks=[tf.train.StopAtStepHook(last_step=100)]\n",
    "  \n",
    "  # new session to distributed learning!\n",
    "  # use this session following this code\n",
    "  with tf.train.MonitoredTrainingSession(master=server.target,\n",
    "                                         is_chief=(task_index == 0),\n",
    "                                         checkpoint_dir=\"./model/train_logs\",\n",
    "                                         hooks=hooks) as mon_sess:\n",
    "    while not mon_sess.should_stop():\n",
    "      # Run a training step asynchronously.\n",
    "      # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n",
    "      # perform *synchronous* training.\n",
    "      # mon_sess.run handles AbortedError in case of preempted PS.\n",
    "      mon_sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job master -> {0 -> localhost:27182}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> localhost:27183, 1 -> localhost:27184}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:27185, 1 -> localhost:27186, 2 -> localhost:27187}\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27182\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27183\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27187\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27184\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27186\n",
      "I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:221] Started server with target: grpc://localhost:27185\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud ml-engine local train \\\n",
    "--package-path=$(pwd)/model \\\n",
    "--module-name=model.test \\\n",
    "--parameter-server-count=2 \\\n",
    "--worker-count=3 \\\n",
    "--distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You got it if you see the message like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to run the ml-engine as JOB, just set the SCALE_TIER to STANDARD_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The others are same as we've done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: simple_model_20170612_090123\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [simple_model_20170612_090123] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe simple_model_20170612_090123\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs simple_model_20170612_090123\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "PROJECT_ID=tensorflowprojects\n",
    "BUCKET=\"gs://${PROJECT_ID}-mlengine\"\n",
    "JOB_NAME=simple_model_$(date +%Y%m%d_%H%M%S)\n",
    "PACKAGE_PATH=$(pwd)/model\n",
    "MODULE_NAME=model.tast\n",
    "STAGING_BUCKET=${BUCKET}\n",
    "JOB_DIR=${BUCKET}/${JOB_NAME}\n",
    "OUTPUT=${BUCKET}/${JOB_NAME}\n",
    "REGION=europe-west1\n",
    "SCALE_TIER=STANDARD_1\n",
    "\n",
    "# Submit job with these settings\n",
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "--package-path=$PACKAGE_PATH \\\n",
    "--module-name=$MODULE_NAME \\\n",
    "--staging-bucket=$STAGING_BUCKET \\\n",
    "--job-dir=$JOB_DIR \\\n",
    "--scale-tier=$SCALE_TIER \\\n",
    "--region=$REGION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, It seems most of necessary things have been written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details about this are in the [documents](https://www.tensorflow.org/deploy/distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
